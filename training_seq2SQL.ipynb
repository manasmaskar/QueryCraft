{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from transformers import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manasmaskar/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the saved tokenizer and initialize the T5 model\n",
    "tokenizer_path = \"../Models/T5Tokenizer/\"  # Update with the correct path to t5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "data_path = \"../data/cleaned_data/cleaned_train_dataset.csv\"  # Update with the correct path\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Split into training and validation sets (e.g., 80% training, 20% validation)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SQLDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract question and SQL query\n",
    "        question = self.data.iloc[idx]['question']\n",
    "        query = self.data.iloc[idx]['query']\n",
    "        \n",
    "        # Tokenize question and query using the saved tokenizer\n",
    "        question_encodings = self.tokenizer(\n",
    "            question, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        query_encodings = self.tokenizer(\n",
    "            query, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': question_encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': question_encodings['attention_mask'].squeeze(),\n",
    "            'labels': query_encodings['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "# Create training and validation DataLoader instances\n",
    "train_dataset = Seq2SQLDataset(train_df, tokenizer)\n",
    "val_dataset = Seq2SQLDataset(val_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=3, lr=5e-5):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print batch loss every 10 batches\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss}\")\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss}\")\n",
    "        \n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manasmaskar/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10/700, Loss: 7.924383640289307\n",
      "Epoch 1, Batch 20/700, Loss: 3.5404655933380127\n",
      "Epoch 1, Batch 30/700, Loss: 1.0771628618240356\n",
      "Epoch 1, Batch 40/700, Loss: 0.67256760597229\n",
      "Epoch 1, Batch 50/700, Loss: 0.7317628264427185\n",
      "Epoch 1, Batch 60/700, Loss: 0.5829547047615051\n",
      "Epoch 1, Batch 70/700, Loss: 0.5261867046356201\n",
      "Epoch 1, Batch 80/700, Loss: 0.4380473792552948\n",
      "Epoch 1, Batch 90/700, Loss: 0.42561864852905273\n",
      "Epoch 1, Batch 100/700, Loss: 0.2960124611854553\n",
      "Epoch 1, Batch 110/700, Loss: 0.2630559504032135\n",
      "Epoch 1, Batch 120/700, Loss: 0.3568367063999176\n",
      "Epoch 1, Batch 130/700, Loss: 0.2967519164085388\n",
      "Epoch 1, Batch 140/700, Loss: 0.3578341007232666\n",
      "Epoch 1, Batch 150/700, Loss: 0.33193260431289673\n",
      "Epoch 1, Batch 160/700, Loss: 0.28751271963119507\n",
      "Epoch 1, Batch 170/700, Loss: 0.2526474595069885\n",
      "Epoch 1, Batch 180/700, Loss: 0.33262985944747925\n",
      "Epoch 1, Batch 190/700, Loss: 0.37403175234794617\n",
      "Epoch 1, Batch 200/700, Loss: 0.2264709770679474\n",
      "Epoch 1, Batch 210/700, Loss: 0.1793224960565567\n",
      "Epoch 1, Batch 220/700, Loss: 0.21678997576236725\n",
      "Epoch 1, Batch 230/700, Loss: 0.21551677584648132\n",
      "Epoch 1, Batch 240/700, Loss: 0.21405793726444244\n",
      "Epoch 1, Batch 250/700, Loss: 0.23529134690761566\n",
      "Epoch 1, Batch 260/700, Loss: 0.1568436622619629\n",
      "Epoch 1, Batch 270/700, Loss: 0.2240435779094696\n",
      "Epoch 1, Batch 280/700, Loss: 0.20282302796840668\n",
      "Epoch 1, Batch 290/700, Loss: 0.22810925543308258\n",
      "Epoch 1, Batch 300/700, Loss: 0.16428227722644806\n",
      "Epoch 1, Batch 310/700, Loss: 0.16293072700500488\n",
      "Epoch 1, Batch 320/700, Loss: 0.18666847050189972\n",
      "Epoch 1, Batch 330/700, Loss: 0.17554308474063873\n",
      "Epoch 1, Batch 340/700, Loss: 0.12899215519428253\n",
      "Epoch 1, Batch 350/700, Loss: 0.17041854560375214\n",
      "Epoch 1, Batch 360/700, Loss: 0.16456834971904755\n",
      "Epoch 1, Batch 370/700, Loss: 0.17879578471183777\n",
      "Epoch 1, Batch 380/700, Loss: 0.21095411479473114\n",
      "Epoch 1, Batch 390/700, Loss: 0.15110273659229279\n",
      "Epoch 1, Batch 400/700, Loss: 0.13520607352256775\n",
      "Epoch 1, Batch 410/700, Loss: 0.28966328501701355\n",
      "Epoch 1, Batch 420/700, Loss: 0.16956844925880432\n",
      "Epoch 1, Batch 430/700, Loss: 0.11132657527923584\n",
      "Epoch 1, Batch 440/700, Loss: 0.18002934753894806\n",
      "Epoch 1, Batch 450/700, Loss: 0.15152835845947266\n",
      "Epoch 1, Batch 460/700, Loss: 0.15926814079284668\n",
      "Epoch 1, Batch 470/700, Loss: 0.11982353031635284\n",
      "Epoch 1, Batch 480/700, Loss: 0.17570288479328156\n",
      "Epoch 1, Batch 490/700, Loss: 0.1791861206293106\n",
      "Epoch 1, Batch 500/700, Loss: 0.09191642701625824\n",
      "Epoch 1, Batch 510/700, Loss: 0.15452538430690765\n",
      "Epoch 1, Batch 520/700, Loss: 0.14729057252407074\n",
      "Epoch 1, Batch 530/700, Loss: 0.14057746529579163\n",
      "Epoch 1, Batch 540/700, Loss: 0.13741348683834076\n",
      "Epoch 1, Batch 550/700, Loss: 0.12271980941295624\n",
      "Epoch 1, Batch 560/700, Loss: 0.12859244644641876\n",
      "Epoch 1, Batch 570/700, Loss: 0.18556749820709229\n",
      "Epoch 1, Batch 580/700, Loss: 0.13403916358947754\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Call the train_model function with your model, train_loader, and val_loader\n",
    "train_model(model, train_loader, val_loader, num_epochs=3, lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define save path\n",
    "model_save_dir = \"../Models/seq2sql_model\"  # Update path as needed\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(model_save_dir)\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
