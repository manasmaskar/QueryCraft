{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer for Seq2SQL\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(item):\n",
    "    question, query = item\n",
    "    input_ids = tokenizer.encode(question, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "    label_ids = tokenizer.encode(query, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "    return input_ids[0], label_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class SQLDataset(Dataset):\n",
    "    def __init__(self, questions, queries, tokenizer, max_length=512):\n",
    "        self.questions = questions\n",
    "        self.queries = queries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        query = self.queries[idx]\n",
    "        \n",
    "        # Tokenize and convert to tensors\n",
    "        question_encodings = self.tokenizer.encode_plus(\n",
    "            question, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        query_encodings = self.tokenizer.encode_plus(\n",
    "            query, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return question_encodings[\"input_ids\"].squeeze(), query_encodings[\"input_ids\"].squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manasmaskar/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 11.202656745910645\n",
      "Epoch: 0, Loss: 4.101990222930908\n",
      "Epoch: 0, Loss: 2.327583074569702\n",
      "Epoch: 0, Loss: 1.8115757703781128\n",
      "Epoch: 0, Loss: 1.5076320171356201\n",
      "Epoch: 0, Loss: 1.5582963228225708\n",
      "Epoch: 0, Loss: 1.3552078008651733\n",
      "Epoch: 0, Loss: 1.3715909719467163\n",
      "Epoch: 0, Loss: 1.6009360551834106\n",
      "Epoch: 0, Loss: 1.2804819345474243\n",
      "Epoch: 0, Loss: 1.128474473953247\n",
      "Epoch: 0, Loss: 0.9469460248947144\n",
      "Epoch: 0, Loss: 0.9332171082496643\n",
      "Epoch: 0, Loss: 1.220102310180664\n",
      "Epoch: 0, Loss: 0.8259848356246948\n",
      "Epoch: 0, Loss: 0.7255359292030334\n",
      "Epoch: 0, Loss: 0.7607753872871399\n",
      "Epoch: 0, Loss: 0.6331817507743835\n",
      "Epoch: 0, Loss: 0.7283965349197388\n",
      "Epoch: 0, Loss: 0.6691262722015381\n",
      "Epoch: 0, Loss: 0.573510468006134\n",
      "Epoch: 0, Loss: 0.5922947525978088\n",
      "Epoch: 0, Loss: 0.4901600480079651\n",
      "Epoch: 0, Loss: 0.4828447997570038\n",
      "Epoch: 0, Loss: 0.42231664061546326\n",
      "Epoch: 0, Loss: 0.5467911958694458\n",
      "Epoch: 0, Loss: 0.4278581738471985\n",
      "Epoch: 0, Loss: 0.4160709083080292\n",
      "Epoch: 0, Loss: 0.4319683611392975\n",
      "Epoch: 0, Loss: 0.5226545333862305\n",
      "Epoch: 0, Loss: 0.41783419251441956\n",
      "Epoch: 0, Loss: 0.9508136510848999\n",
      "Epoch: 0, Loss: 0.32072314620018005\n",
      "Epoch: 0, Loss: 0.5144516825675964\n",
      "Epoch: 0, Loss: 0.29693448543548584\n",
      "Epoch: 0, Loss: 0.699004054069519\n",
      "Epoch: 0, Loss: 0.41662999987602234\n",
      "Epoch: 0, Loss: 0.41905391216278076\n",
      "Epoch: 0, Loss: 0.3571168780326843\n",
      "Epoch: 0, Loss: 0.3929261565208435\n",
      "Epoch: 0, Loss: 0.3943980038166046\n",
      "Epoch: 0, Loss: 0.697345495223999\n",
      "Epoch: 0, Loss: 0.38181272149086\n",
      "Epoch: 0, Loss: 0.38318660855293274\n",
      "Epoch: 0, Loss: 0.3611373007297516\n",
      "Epoch: 0, Loss: 0.3498169779777527\n",
      "Epoch: 0, Loss: 0.24106334149837494\n",
      "Epoch: 0, Loss: 0.40153202414512634\n",
      "Epoch: 0, Loss: 0.42821747064590454\n",
      "Epoch: 0, Loss: 0.3287605941295624\n",
      "Epoch: 0, Loss: 0.3840506076812744\n",
      "Epoch: 0, Loss: 0.4358406960964203\n",
      "Epoch: 0, Loss: 0.35302114486694336\n",
      "Epoch: 0, Loss: 0.29623401165008545\n",
      "Epoch: 0, Loss: 0.2477642297744751\n",
      "Epoch: 0, Loss: 0.33920708298683167\n",
      "Epoch: 0, Loss: 0.35490867495536804\n",
      "Epoch: 0, Loss: 0.37385234236717224\n",
      "Epoch: 0, Loss: 0.2923435568809509\n",
      "Epoch: 0, Loss: 0.34739255905151367\n",
      "Epoch: 0, Loss: 0.27245819568634033\n",
      "Epoch: 0, Loss: 0.28038954734802246\n",
      "Epoch: 0, Loss: 0.4612276256084442\n",
      "Epoch: 0, Loss: 0.384708046913147\n",
      "Epoch: 0, Loss: 0.2985002398490906\n",
      "Epoch: 0, Loss: 0.3355027139186859\n",
      "Epoch: 0, Loss: 0.3772498369216919\n",
      "Epoch: 0, Loss: 0.4049080014228821\n",
      "Epoch: 0, Loss: 0.28058505058288574\n",
      "Epoch: 0, Loss: 0.2847832143306732\n",
      "Epoch: 0, Loss: 0.39418017864227295\n",
      "Epoch: 0, Loss: 0.27924972772598267\n",
      "Epoch: 0, Loss: 0.39713215827941895\n",
      "Epoch: 0, Loss: 0.2882291376590729\n",
      "Epoch: 0, Loss: 0.297382652759552\n",
      "Epoch: 0, Loss: 0.28253641724586487\n",
      "Epoch: 0, Loss: 0.2585800886154175\n",
      "Epoch: 0, Loss: 0.277120977640152\n",
      "Epoch: 0, Loss: 0.22370950877666473\n",
      "Epoch: 0, Loss: 0.25310665369033813\n",
      "Epoch: 0, Loss: 0.30710235238075256\n",
      "Epoch: 0, Loss: 0.4422195255756378\n",
      "Epoch: 0, Loss: 0.2834188640117645\n",
      "Epoch: 0, Loss: 0.20094498991966248\n",
      "Epoch: 0, Loss: 0.3874554932117462\n",
      "Epoch: 0, Loss: 0.24146407842636108\n",
      "Epoch: 0, Loss: 0.3046383559703827\n",
      "Epoch: 0, Loss: 0.29664379358291626\n",
      "Epoch: 0, Loss: 0.39040887355804443\n",
      "Epoch: 0, Loss: 0.4772945046424866\n",
      "Epoch: 0, Loss: 0.30701661109924316\n",
      "Epoch: 0, Loss: 0.24417927861213684\n",
      "Epoch: 0, Loss: 0.31925007700920105\n",
      "Epoch: 0, Loss: 0.3848547339439392\n",
      "Epoch: 0, Loss: 0.2622879445552826\n",
      "Epoch: 0, Loss: 0.30995482206344604\n",
      "Epoch: 0, Loss: 0.277133047580719\n",
      "Epoch: 0, Loss: 0.2910507321357727\n",
      "Epoch: 0, Loss: 0.4877776503562927\n",
      "Epoch: 0, Loss: 0.3472869396209717\n",
      "Epoch: 0, Loss: 0.2740887403488159\n",
      "Epoch: 0, Loss: 0.2510179579257965\n",
      "Epoch: 0, Loss: 0.17464090883731842\n",
      "Epoch: 0, Loss: 0.16871246695518494\n",
      "Epoch: 0, Loss: 0.19596336781978607\n",
      "Epoch: 0, Loss: 0.3219754099845886\n",
      "Epoch: 0, Loss: 0.20796123147010803\n",
      "Epoch: 0, Loss: 0.3820400536060333\n",
      "Epoch: 0, Loss: 0.23286913335323334\n",
      "Epoch: 0, Loss: 0.2136751264333725\n",
      "Epoch: 0, Loss: 0.2537411153316498\n",
      "Epoch: 0, Loss: 0.27833718061447144\n",
      "Epoch: 0, Loss: 0.22333241999149323\n",
      "Epoch: 0, Loss: 0.2724080979824066\n",
      "Epoch: 0, Loss: 0.25730597972869873\n",
      "Epoch: 0, Loss: 0.3112185597419739\n",
      "Epoch: 0, Loss: 0.2598976194858551\n",
      "Epoch: 0, Loss: 0.27006614208221436\n",
      "Epoch: 0, Loss: 0.32721084356307983\n",
      "Epoch: 0, Loss: 0.23122282326221466\n",
      "Epoch: 0, Loss: 0.19791464507579803\n",
      "Epoch: 0, Loss: 0.21266041696071625\n",
      "Epoch: 0, Loss: 0.23859335482120514\n",
      "Epoch: 0, Loss: 0.27841001749038696\n",
      "Epoch: 0, Loss: 0.21966128051280975\n",
      "Epoch: 0, Loss: 0.18818043172359467\n",
      "Epoch: 0, Loss: 0.2685001790523529\n",
      "Epoch: 0, Loss: 0.20054373145103455\n",
      "Epoch: 0, Loss: 0.22211013734340668\n",
      "Epoch: 0, Loss: 0.26879191398620605\n",
      "Epoch: 0, Loss: 0.2144671082496643\n",
      "Epoch: 0, Loss: 0.2446163445711136\n",
      "Epoch: 0, Loss: 0.23239769041538239\n",
      "Epoch: 0, Loss: 0.21052657067775726\n",
      "Epoch: 0, Loss: 0.2608340382575989\n",
      "Epoch: 0, Loss: 0.2233910858631134\n",
      "Epoch: 0, Loss: 0.18371957540512085\n",
      "Epoch: 0, Loss: 0.23024523258209229\n",
      "Epoch: 0, Loss: 0.29883483052253723\n",
      "Epoch: 0, Loss: 0.3325668275356293\n",
      "Epoch: 0, Loss: 0.18851836025714874\n",
      "Epoch: 0, Loss: 0.23725654184818268\n",
      "Epoch: 0, Loss: 0.3149130046367645\n",
      "Epoch: 0, Loss: 0.26423123478889465\n",
      "Epoch: 0, Loss: 0.2784358263015747\n",
      "Epoch: 0, Loss: 0.24922719597816467\n",
      "Epoch: 0, Loss: 0.18132098019123077\n",
      "Epoch: 0, Loss: 0.3055083155632019\n",
      "Epoch: 0, Loss: 0.18712112307548523\n",
      "Epoch: 0, Loss: 0.2595076858997345\n",
      "Epoch: 0, Loss: 0.2720353305339813\n",
      "Epoch: 0, Loss: 0.18099792301654816\n",
      "Epoch: 0, Loss: 0.22575770318508148\n",
      "Epoch: 0, Loss: 0.16536034643650055\n",
      "Epoch: 0, Loss: 0.19017083942890167\n",
      "Epoch: 0, Loss: 0.248687744140625\n",
      "Epoch: 0, Loss: 0.1517103612422943\n",
      "Epoch: 0, Loss: 0.2276584357023239\n",
      "Epoch: 0, Loss: 0.17669518291950226\n",
      "Epoch: 0, Loss: 0.28946632146835327\n",
      "Epoch: 0, Loss: 0.17098602652549744\n",
      "Epoch: 0, Loss: 0.25347432494163513\n",
      "Epoch: 0, Loss: 0.153926283121109\n",
      "Epoch: 0, Loss: 0.1871749758720398\n",
      "Epoch: 0, Loss: 0.15870237350463867\n",
      "Epoch: 0, Loss: 0.29004696011543274\n",
      "Epoch: 0, Loss: 0.2582123577594757\n",
      "Epoch: 0, Loss: 0.25422993302345276\n",
      "Epoch: 0, Loss: 0.18251293897628784\n",
      "Epoch: 0, Loss: 0.18395736813545227\n",
      "Epoch: 0, Loss: 0.1574023962020874\n",
      "Epoch: 0, Loss: 0.16895060241222382\n",
      "Epoch: 0, Loss: 0.1874081939458847\n",
      "Epoch: 0, Loss: 0.1643940806388855\n",
      "Epoch: 0, Loss: 0.1778268963098526\n",
      "Epoch: 0, Loss: 0.264526903629303\n",
      "Epoch: 0, Loss: 0.16134582459926605\n",
      "Epoch: 0, Loss: 0.15169312059879303\n",
      "Epoch: 0, Loss: 0.17209121584892273\n",
      "Epoch: 0, Loss: 0.1711091250181198\n",
      "Epoch: 0, Loss: 0.19578376412391663\n",
      "Epoch: 0, Loss: 0.23152753710746765\n",
      "Epoch: 0, Loss: 0.19142669439315796\n",
      "Epoch: 0, Loss: 0.158330038189888\n",
      "Epoch: 0, Loss: 0.17089301347732544\n",
      "Epoch: 0, Loss: 0.20990446209907532\n",
      "Epoch: 0, Loss: 0.17644880712032318\n",
      "Epoch: 0, Loss: 0.1595093309879303\n",
      "Epoch: 0, Loss: 0.19044801592826843\n",
      "Epoch: 0, Loss: 0.21586470305919647\n",
      "Epoch: 0, Loss: 0.20021739602088928\n",
      "Epoch: 0, Loss: 0.19255058467388153\n",
      "Epoch: 0, Loss: 0.23271213471889496\n",
      "Epoch: 0, Loss: 0.203280508518219\n",
      "Epoch: 0, Loss: 0.12449204176664352\n",
      "Epoch: 0, Loss: 0.24786940217018127\n",
      "Epoch: 0, Loss: 0.16681136190891266\n",
      "Epoch: 0, Loss: 0.16691939532756805\n",
      "Epoch: 0, Loss: 0.18450003862380981\n",
      "Epoch: 0, Loss: 0.22490449249744415\n",
      "Epoch: 0, Loss: 0.1805168241262436\n",
      "Epoch: 0, Loss: 0.19376370310783386\n",
      "Epoch: 0, Loss: 0.28194311261177063\n",
      "Epoch: 0, Loss: 0.152864471077919\n",
      "Epoch: 0, Loss: 0.20340973138809204\n",
      "Epoch: 0, Loss: 0.14367705583572388\n",
      "Epoch: 0, Loss: 0.1632719337940216\n",
      "Epoch: 0, Loss: 0.25239071249961853\n",
      "Epoch: 0, Loss: 0.14883831143379211\n",
      "Epoch: 0, Loss: 0.2260800153017044\n",
      "Epoch: 0, Loss: 0.19168877601623535\n",
      "Epoch: 0, Loss: 0.17268924415111542\n",
      "Epoch: 0, Loss: 0.1752181500196457\n",
      "Epoch: 0, Loss: 0.181660994887352\n",
      "Epoch: 0, Loss: 0.20007623732089996\n",
      "Epoch: 0, Loss: 0.155251607298851\n",
      "Epoch: 0, Loss: 0.13200657069683075\n",
      "Epoch: 0, Loss: 0.1769523173570633\n",
      "Epoch: 0, Loss: 0.14886541664600372\n",
      "Epoch: 0, Loss: 0.16070552170276642\n",
      "Epoch: 0, Loss: 0.16050024330615997\n",
      "Epoch: 0, Loss: 0.1319013386964798\n",
      "Epoch: 0, Loss: 0.2431541383266449\n",
      "Epoch: 0, Loss: 0.16392385959625244\n",
      "Epoch: 0, Loss: 0.19667254388332367\n",
      "Epoch: 0, Loss: 0.13228929042816162\n",
      "Epoch: 0, Loss: 0.1633489727973938\n",
      "Epoch: 0, Loss: 0.1646360158920288\n",
      "Epoch: 0, Loss: 0.1652935892343521\n",
      "Epoch: 0, Loss: 0.12236648052930832\n",
      "Epoch: 0, Loss: 0.19369222223758698\n",
      "Epoch: 0, Loss: 0.15195466578006744\n",
      "Epoch: 0, Loss: 0.17277592420578003\n",
      "Epoch: 0, Loss: 0.17154251039028168\n",
      "Epoch: 0, Loss: 0.17326049506664276\n",
      "Epoch: 0, Loss: 0.12496583163738251\n",
      "Epoch: 0, Loss: 0.12185362726449966\n",
      "Epoch: 0, Loss: 0.16735640168190002\n",
      "Epoch: 0, Loss: 0.23169316351413727\n",
      "Epoch: 0, Loss: 0.13189885020256042\n",
      "Epoch: 0, Loss: 0.14864706993103027\n",
      "Epoch: 0, Loss: 0.25653618574142456\n",
      "Epoch: 0, Loss: 0.18614721298217773\n",
      "Epoch: 0, Loss: 0.14703966677188873\n",
      "Epoch: 0, Loss: 0.12355479598045349\n",
      "Epoch: 0, Loss: 0.13681010901927948\n",
      "Epoch: 0, Loss: 0.2048453986644745\n",
      "Epoch: 0, Loss: 0.1576540321111679\n",
      "Epoch: 0, Loss: 0.14718861877918243\n",
      "Epoch: 0, Loss: 0.19620372354984283\n",
      "Epoch: 0, Loss: 0.20693424344062805\n",
      "Epoch: 0, Loss: 0.20002120733261108\n",
      "Epoch: 0, Loss: 0.16676831245422363\n",
      "Epoch: 0, Loss: 0.13080056011676788\n",
      "Epoch: 0, Loss: 0.15986402332782745\n",
      "Epoch: 0, Loss: 0.152263805270195\n",
      "Epoch: 0, Loss: 0.3038221597671509\n",
      "Epoch: 0, Loss: 0.15421302616596222\n",
      "Epoch: 0, Loss: 0.20225508511066437\n",
      "Epoch: 0, Loss: 0.13408534228801727\n",
      "Epoch: 0, Loss: 0.20332303643226624\n",
      "Epoch: 0, Loss: 0.12492307275533676\n",
      "Epoch: 0, Loss: 0.16095297038555145\n",
      "Epoch: 0, Loss: 0.15209205448627472\n",
      "Epoch: 0, Loss: 0.2421499788761139\n",
      "Epoch: 0, Loss: 0.1410873681306839\n",
      "Epoch: 0, Loss: 0.14272379875183105\n",
      "Epoch: 0, Loss: 0.15675976872444153\n",
      "Epoch: 0, Loss: 0.1546972543001175\n",
      "Epoch: 0, Loss: 0.1738697588443756\n",
      "Epoch: 0, Loss: 0.12858319282531738\n",
      "Epoch: 0, Loss: 0.18311141431331635\n",
      "Epoch: 0, Loss: 0.21416451036930084\n",
      "Epoch: 0, Loss: 0.11341803520917892\n",
      "Epoch: 0, Loss: 0.12300179153680801\n",
      "Epoch: 0, Loss: 0.10665617138147354\n",
      "Epoch: 0, Loss: 0.1844937801361084\n",
      "Epoch: 0, Loss: 0.15049435198307037\n",
      "Epoch: 0, Loss: 0.14182190597057343\n",
      "Epoch: 0, Loss: 0.1501789540052414\n",
      "Epoch: 0, Loss: 0.174192413687706\n",
      "Epoch: 0, Loss: 0.1948503851890564\n",
      "Epoch: 0, Loss: 0.16301390528678894\n",
      "Epoch: 0, Loss: 0.17909912765026093\n",
      "Epoch: 0, Loss: 0.13425129652023315\n",
      "Epoch: 0, Loss: 0.16751211881637573\n",
      "Epoch: 0, Loss: 0.16472800076007843\n",
      "Epoch: 0, Loss: 0.14860519766807556\n",
      "Epoch: 0, Loss: 0.14177118241786957\n",
      "Epoch: 0, Loss: 0.15697336196899414\n",
      "Epoch: 0, Loss: 0.1272544264793396\n",
      "Epoch: 0, Loss: 0.13942943513393402\n",
      "Epoch: 0, Loss: 0.12945117056369781\n",
      "Epoch: 0, Loss: 0.19067473709583282\n",
      "Epoch: 0, Loss: 0.12559202313423157\n",
      "Epoch: 0, Loss: 0.10199443250894547\n",
      "Epoch: 0, Loss: 0.16132012009620667\n",
      "Epoch: 0, Loss: 0.14540976285934448\n",
      "Epoch: 0, Loss: 0.17916448414325714\n",
      "Epoch: 0, Loss: 0.15331929922103882\n",
      "Epoch: 0, Loss: 0.13557229936122894\n",
      "Epoch: 0, Loss: 0.13453587889671326\n",
      "Epoch: 0, Loss: 0.12729068100452423\n",
      "Epoch: 0, Loss: 0.16822926700115204\n",
      "Epoch: 0, Loss: 0.18327055871486664\n",
      "Epoch: 0, Loss: 0.13706161081790924\n",
      "Epoch: 0, Loss: 0.14839783310890198\n",
      "Epoch: 0, Loss: 0.12740415334701538\n",
      "Epoch: 0, Loss: 0.12341684848070145\n",
      "Epoch: 0, Loss: 0.1298227459192276\n",
      "Epoch: 0, Loss: 0.08697569370269775\n",
      "Epoch: 0, Loss: 0.12121963500976562\n",
      "Epoch: 0, Loss: 0.15464681386947632\n",
      "Epoch: 0, Loss: 0.18272516131401062\n",
      "Epoch: 0, Loss: 0.11077134311199188\n",
      "Epoch: 0, Loss: 0.10997865349054337\n",
      "Epoch: 0, Loss: 0.13334216177463531\n",
      "Epoch: 0, Loss: 0.11702115088701248\n",
      "Epoch: 0, Loss: 0.1390688717365265\n",
      "Epoch: 0, Loss: 0.08974417299032211\n",
      "Epoch: 0, Loss: 0.12614057958126068\n",
      "Epoch: 0, Loss: 0.11819092184305191\n",
      "Epoch: 0, Loss: 0.11112165451049805\n",
      "Epoch: 0, Loss: 0.15998925268650055\n",
      "Epoch: 0, Loss: 0.12504498660564423\n",
      "Epoch: 0, Loss: 0.10026606172323227\n",
      "Epoch: 0, Loss: 0.1471182405948639\n",
      "Epoch: 0, Loss: 0.11008771508932114\n",
      "Epoch: 0, Loss: 0.09480326622724533\n",
      "Epoch: 0, Loss: 0.16875535249710083\n",
      "Epoch: 0, Loss: 0.11408627033233643\n",
      "Epoch: 0, Loss: 0.10762132704257965\n",
      "Epoch: 0, Loss: 0.09594843536615372\n",
      "Epoch: 0, Loss: 0.14185784757137299\n",
      "Epoch: 0, Loss: 0.10457605123519897\n",
      "Epoch: 0, Loss: 0.11461497843265533\n",
      "Epoch: 0, Loss: 0.1518612951040268\n",
      "Epoch: 0, Loss: 0.09697704017162323\n",
      "Epoch: 0, Loss: 0.12565074861049652\n",
      "Epoch: 0, Loss: 0.16404207050800323\n",
      "Epoch: 0, Loss: 0.12768958508968353\n",
      "Epoch: 0, Loss: 0.11286503076553345\n",
      "Epoch: 0, Loss: 0.1269526332616806\n",
      "Epoch: 0, Loss: 0.20201422274112701\n",
      "Epoch: 0, Loss: 0.10239063948392868\n",
      "Epoch: 0, Loss: 0.13394154608249664\n",
      "Epoch: 0, Loss: 0.1153675839304924\n",
      "Epoch: 0, Loss: 0.19399669766426086\n",
      "Epoch: 0, Loss: 0.13671927154064178\n",
      "Epoch: 0, Loss: 0.13186295330524445\n",
      "Epoch: 0, Loss: 0.15926703810691833\n",
      "Epoch: 0, Loss: 0.09431648999452591\n",
      "Epoch: 0, Loss: 0.1266365647315979\n",
      "Epoch: 0, Loss: 0.15302126109600067\n",
      "Epoch: 0, Loss: 0.13083674013614655\n",
      "Epoch: 0, Loss: 0.14738161861896515\n",
      "Epoch: 0, Loss: 0.10987976938486099\n",
      "Epoch: 0, Loss: 0.13093486428260803\n",
      "Epoch: 0, Loss: 0.1915263831615448\n",
      "Epoch: 0, Loss: 0.12306910753250122\n",
      "Epoch: 0, Loss: 0.07618055492639542\n",
      "Epoch: 0, Loss: 0.10049540549516678\n",
      "Epoch: 0, Loss: 0.14704811573028564\n",
      "Epoch: 0, Loss: 0.13677844405174255\n",
      "Epoch: 0, Loss: 0.12602803111076355\n",
      "Epoch: 0, Loss: 0.15184231102466583\n",
      "Epoch: 0, Loss: 0.1217040866613388\n",
      "Epoch: 0, Loss: 0.129989892244339\n",
      "Epoch: 0, Loss: 0.18863487243652344\n",
      "Epoch: 0, Loss: 0.10864377021789551\n",
      "Epoch: 0, Loss: 0.1220051497220993\n",
      "Epoch: 0, Loss: 0.1285484880208969\n",
      "Epoch: 0, Loss: 0.13408072292804718\n",
      "Epoch: 0, Loss: 0.12172766774892807\n",
      "Epoch: 0, Loss: 0.07415366917848587\n",
      "Epoch: 0, Loss: 0.13103675842285156\n",
      "Epoch: 0, Loss: 0.18330760300159454\n",
      "Epoch: 0, Loss: 0.11830122023820877\n",
      "Epoch: 0, Loss: 0.09228482097387314\n",
      "Epoch: 0, Loss: 0.06471917778253555\n",
      "Epoch: 0, Loss: 0.14444182813167572\n",
      "Epoch: 0, Loss: 0.15116018056869507\n",
      "Epoch: 0, Loss: 0.1163124293088913\n",
      "Epoch: 0, Loss: 0.1004738062620163\n",
      "Epoch: 0, Loss: 0.10905653983354568\n",
      "Epoch: 0, Loss: 0.06898167729377747\n",
      "Epoch: 0, Loss: 0.14897224307060242\n",
      "Epoch: 0, Loss: 0.11990572512149811\n",
      "Epoch: 0, Loss: 0.10482624173164368\n",
      "Epoch: 0, Loss: 0.12287446856498718\n",
      "Epoch: 0, Loss: 0.12086745351552963\n",
      "Epoch: 0, Loss: 0.0990927517414093\n",
      "Epoch: 0, Loss: 0.153361976146698\n",
      "Epoch: 0, Loss: 0.1272609531879425\n",
      "Epoch: 0, Loss: 0.1643926501274109\n",
      "Epoch: 0, Loss: 0.09736688435077667\n",
      "Epoch: 0, Loss: 0.15672703087329865\n",
      "Epoch: 0, Loss: 0.10395753383636475\n",
      "Epoch: 0, Loss: 0.07906748354434967\n",
      "Epoch: 0, Loss: 0.0945846289396286\n",
      "Epoch: 0, Loss: 0.1623072624206543\n",
      "Epoch: 0, Loss: 0.08070503920316696\n",
      "Epoch: 0, Loss: 0.10939264297485352\n",
      "Epoch: 0, Loss: 0.10555315762758255\n",
      "Epoch: 0, Loss: 0.08900057524442673\n",
      "Epoch: 0, Loss: 0.0991855040192604\n",
      "Epoch: 0, Loss: 0.1545201689004898\n",
      "Epoch: 0, Loss: 0.11705444008111954\n",
      "Epoch: 0, Loss: 0.13458892703056335\n",
      "Epoch: 0, Loss: 0.12472978979349136\n",
      "Epoch: 0, Loss: 0.0791022852063179\n",
      "Epoch: 0, Loss: 0.17082913219928741\n",
      "Epoch: 0, Loss: 0.15401716530323029\n",
      "Epoch: 0, Loss: 0.14089246094226837\n",
      "Epoch: 0, Loss: 0.20266611874103546\n",
      "Epoch: 0, Loss: 0.19544720649719238\n",
      "Epoch: 0, Loss: 0.14568346738815308\n",
      "Epoch: 0, Loss: 0.1494363248348236\n",
      "Epoch: 0, Loss: 0.09613244235515594\n",
      "Epoch: 0, Loss: 0.053921110928058624\n",
      "Epoch: 0, Loss: 0.09015100449323654\n",
      "Epoch: 0, Loss: 0.08641885966062546\n",
      "Epoch: 0, Loss: 0.11522890627384186\n",
      "Epoch: 0, Loss: 0.0985848680138588\n",
      "Epoch: 0, Loss: 0.10639477521181107\n",
      "Epoch: 0, Loss: 0.11678922921419144\n",
      "Epoch: 0, Loss: 0.1291404515504837\n",
      "Epoch: 0, Loss: 0.10436362028121948\n",
      "Epoch: 0, Loss: 0.13192497193813324\n",
      "Epoch: 0, Loss: 0.1073082983493805\n",
      "Epoch: 0, Loss: 0.10282891988754272\n",
      "Epoch: 0, Loss: 0.11157885193824768\n",
      "Epoch: 0, Loss: 0.12007270008325577\n",
      "Epoch: 0, Loss: 0.1191537007689476\n",
      "Epoch: 0, Loss: 0.13422909379005432\n",
      "Epoch: 0, Loss: 0.06264714896678925\n",
      "Epoch: 0, Loss: 0.10372379422187805\n",
      "Epoch: 0, Loss: 0.10957012325525284\n",
      "Epoch: 0, Loss: 0.08600243926048279\n",
      "Epoch: 0, Loss: 0.1252470463514328\n",
      "Epoch: 0, Loss: 0.10708926618099213\n",
      "Epoch: 0, Loss: 0.11813199520111084\n",
      "Epoch: 0, Loss: 0.05295298248529434\n",
      "Epoch: 0, Loss: 0.14232201874256134\n",
      "Epoch: 0, Loss: 0.06685253232717514\n",
      "Epoch: 0, Loss: 0.15371035039424896\n",
      "Epoch: 0, Loss: 0.0948990136384964\n",
      "Epoch: 0, Loss: 0.09106267988681793\n",
      "Epoch: 0, Loss: 0.178655207157135\n",
      "Epoch: 0, Loss: 0.12146448343992233\n",
      "Epoch: 0, Loss: 0.07531382888555527\n",
      "Epoch: 0, Loss: 0.0921487957239151\n",
      "Epoch: 0, Loss: 0.10286730527877808\n",
      "Epoch: 0, Loss: 0.1095888614654541\n",
      "Epoch: 0, Loss: 0.11819440871477127\n",
      "Epoch: 0, Loss: 0.09570898860692978\n",
      "Epoch: 0, Loss: 0.15392328798770905\n",
      "Epoch: 0, Loss: 0.0726381242275238\n",
      "Epoch: 0, Loss: 0.10028646141290665\n",
      "Epoch: 0, Loss: 0.12358273565769196\n",
      "Epoch: 0, Loss: 0.10658471286296844\n",
      "Epoch: 0, Loss: 0.09291718900203705\n",
      "Epoch: 0, Loss: 0.11247764527797699\n",
      "Epoch: 0, Loss: 0.12386906892061234\n",
      "Epoch: 0, Loss: 0.12919361889362335\n",
      "Epoch: 0, Loss: 0.0859251469373703\n",
      "Epoch: 0, Loss: 0.08727949857711792\n",
      "Epoch: 0, Loss: 0.11505535989999771\n",
      "Epoch: 0, Loss: 0.10838048905134201\n",
      "Epoch: 0, Loss: 0.08944836258888245\n",
      "Epoch: 0, Loss: 0.146998792886734\n",
      "Epoch: 0, Loss: 0.1082928404211998\n",
      "Epoch: 0, Loss: 0.1000036969780922\n",
      "Epoch: 0, Loss: 0.09257657825946808\n",
      "Epoch: 0, Loss: 0.08003105223178864\n",
      "Epoch: 0, Loss: 0.12875354290008545\n",
      "Epoch: 0, Loss: 0.18381430208683014\n",
      "Epoch: 0, Loss: 0.10470433533191681\n",
      "Epoch: 0, Loss: 0.0927809327840805\n",
      "Epoch: 0, Loss: 0.10378976911306381\n",
      "Epoch: 0, Loss: 0.1250714659690857\n",
      "Epoch: 0, Loss: 0.12110232561826706\n",
      "Epoch: 0, Loss: 0.09569939970970154\n",
      "Epoch: 0, Loss: 0.0955684557557106\n",
      "Epoch: 0, Loss: 0.146304652094841\n",
      "Epoch: 0, Loss: 0.13371264934539795\n",
      "Epoch: 0, Loss: 0.1336432844400406\n",
      "Epoch: 0, Loss: 0.10551204532384872\n",
      "Epoch: 0, Loss: 0.11537893861532211\n",
      "Epoch: 0, Loss: 0.14308252930641174\n",
      "Epoch: 0, Loss: 0.10251966118812561\n",
      "Epoch: 0, Loss: 0.06949969381093979\n",
      "Epoch: 0, Loss: 0.1249428242444992\n",
      "Epoch: 0, Loss: 0.14890770614147186\n",
      "Epoch: 0, Loss: 0.12045866250991821\n",
      "Epoch: 0, Loss: 0.07936196774244308\n",
      "Epoch: 0, Loss: 0.08323772996664047\n",
      "Epoch: 0, Loss: 0.10286446660757065\n",
      "Epoch: 0, Loss: 0.11667662858963013\n",
      "Epoch: 0, Loss: 0.10083512216806412\n",
      "Epoch: 0, Loss: 0.0790414959192276\n",
      "Epoch: 0, Loss: 0.13882969319820404\n",
      "Epoch: 0, Loss: 0.09959124773740768\n",
      "Epoch: 0, Loss: 0.09418104588985443\n",
      "Epoch: 0, Loss: 0.0998869463801384\n",
      "Epoch: 0, Loss: 0.07783706486225128\n",
      "Epoch: 0, Loss: 0.15128719806671143\n",
      "Epoch: 0, Loss: 0.08190770447254181\n",
      "Epoch: 0, Loss: 0.10932905226945877\n",
      "Epoch: 0, Loss: 0.10869928449392319\n",
      "Epoch: 0, Loss: 0.07619455456733704\n",
      "Epoch: 0, Loss: 0.10395484417676926\n",
      "Epoch: 0, Loss: 0.07054871320724487\n",
      "Epoch: 0, Loss: 0.13822489976882935\n",
      "Epoch: 0, Loss: 0.0775722786784172\n",
      "Epoch: 0, Loss: 0.08678177744150162\n",
      "Epoch: 0, Loss: 0.06835371255874634\n",
      "Epoch: 0, Loss: 0.09594908356666565\n",
      "Epoch: 0, Loss: 0.07367219775915146\n",
      "Epoch: 0, Loss: 0.13337752223014832\n",
      "Epoch: 0, Loss: 0.07216556370258331\n",
      "Epoch: 0, Loss: 0.09794045984745026\n",
      "Epoch: 0, Loss: 0.10264718532562256\n",
      "Epoch: 0, Loss: 0.13318368792533875\n",
      "Epoch: 0, Loss: 0.10284489393234253\n",
      "Epoch: 0, Loss: 0.09385515749454498\n",
      "Epoch: 0, Loss: 0.10907567292451859\n",
      "Epoch: 0, Loss: 0.07944533973932266\n",
      "Epoch: 0, Loss: 0.14682309329509735\n",
      "Epoch: 0, Loss: 0.08577697724103928\n",
      "Epoch: 0, Loss: 0.09854311496019363\n",
      "Epoch: 0, Loss: 0.08440444618463516\n",
      "Epoch: 0, Loss: 0.09235750883817673\n",
      "Epoch: 0, Loss: 0.12266598641872406\n",
      "Epoch: 0, Loss: 0.09579744189977646\n",
      "Epoch: 0, Loss: 0.09118200093507767\n",
      "Epoch: 0, Loss: 0.0995120257139206\n",
      "Epoch: 0, Loss: 0.1266019642353058\n",
      "Epoch: 0, Loss: 0.09158172458410263\n",
      "Epoch: 0, Loss: 0.08060339093208313\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m         \u001b[39m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m         loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     38\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m \u001b[39m# Create directory for saving the model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load the cleaned datasets\n",
    "    train_dataset_path = \"../data/cleaned_data/cleaned_train_dataset.csv\"  # Update with your path\n",
    "    validation_dataset_path = \"../data/cleaned_data/cleaned_validation_dataset.csv\"\n",
    "\n",
    "    train_df = pd.read_csv(train_dataset_path)\n",
    "    validation_df = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "    # Prepare the data\n",
    "    questions = train_df['question'].tolist()\n",
    "    queries = train_df['query'].tolist()\n",
    "\n",
    "    # Initialize DataLoader with the updated SQLDataset\n",
    "    train_dataset = SQLDataset(questions, queries, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Define the Seq2SQL model\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    model.train()\n",
    "\n",
    "    # Training the model\n",
    "    num_epochs = 3  # Set your number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            questions, queries = batch  # Each is now a tensor\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=questions, labels=queries)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = outputs.loss\n",
    "            print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Create directory for saving the model\n",
    "    model_save_dir = \"../Models/seq2sql_model\"\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save_pretrained(model_save_dir)\n",
    "    tokenizer.save_pretrained(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manasmaskar/anaconda3/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Define the model save directory\n",
    "model_save_dir = \"../Models/seq2sql_model\"  # Update this path as needed\n",
    "\n",
    "# Initialize the model and tokenizer (ensure you have the same architecture as before)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')  # Ensure this matches your training setup\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "os.makedirs(model_save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "model.save_pretrained(model_save_dir)\n",
    "tokenizer.save_pretrained(model_save_dir)\n",
    "\n",
    "print(\"Model and tokenizer saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the name of the employees who work in the sales department?\n",
      "Predicted SQL: What is the name of the employees who work in the sales department?\n",
      "--------------------------------------------------\n",
      "Question: How many customers made a purchase last month?\n",
      "Predicted SQL: How many customers made a purchase last month?\n",
      "--------------------------------------------------\n",
      "Question: List all products that are currently in stock.\n",
      "Predicted SQL: All products that are currently in stock.\n",
      "--------------------------------------------------\n",
      "Question: How much revenue did the business owner, Manas generate\n",
      "Predicted SQL: , Manas generate?, Manas generates? owner, Man\n",
      "--------------------------------------------------\n",
      "Question: Show the names of customers who have never made a purchase.\n",
      "Predicted SQL: Show the names of customers who have never made a purchase. Show the names of customers\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model_save_dir = \"../Models/seq2sql_model\"  # Update with your path\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_save_dir)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_save_dir)\n",
    "\n",
    "# Define 5 sample input questions\n",
    "sample_questions = [\n",
    "    \"What is the name of the employees who work in the sales department?\",\n",
    "    \"How many customers made a purchase last month?\",\n",
    "    \"List all products that are currently in stock.\",\n",
    "    \"How much revenue did the business owner, Manas generate\",\n",
    "    \"Show the names of customers who have never made a purchase.\"\n",
    "]\n",
    "\n",
    "# Function to generate SQL query from a question\n",
    "def generate_sql(question):\n",
    "    input_ids = tokenizer.encode(question, return_tensors=\"pt\")\n",
    "    output_ids = model.generate(input_ids)\n",
    "    sql_query = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return sql_query\n",
    "\n",
    "# Evaluate the model on the sample questions\n",
    "predicted_queries = [generate_sql(q) for q in sample_questions]\n",
    "\n",
    "# Print the results\n",
    "for question, predicted in zip(sample_questions, predicted_queries):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted SQL: {predicted}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
